{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mS5-Kedv7U4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import tarfile\n",
        "import json\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "import re\n",
        "import sys\n",
        "import argparse\n",
        "import sqlite3\n",
        "import hashlib\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 1. CORE CONFIGURATION ---\n",
        "# ==============================================================================\n",
        "TARGET_MAX_VM_USAGE_GB = 70\n",
        "REPACKAGE_CHUNK_SIZE_GB = 15\n",
        "MAX_SAFE_ARCHIVE_SIZE_GB = 25\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/TakeoutProject\"\n",
        "DB_PATH = os.path.join(BASE_DIR, \"takeout_archive.db\")\n",
        "CONFIG = {\n",
        "    \"SOURCE_ARCHIVES_DIR\": os.path.join(BASE_DIR, \"00-ALL-ARCHIVES/\"),\n",
        "    \"PROCESSING_STAGING_DIR\": os.path.join(BASE_DIR, \"01-PROCESSING-STAGING/\"),\n",
        "    \"ORGANIZED_DIR\": os.path.join(BASE_DIR, \"03-organized/My-Photos/\"),\n",
        "    \"TRASH_DIR\": os.path.join(BASE_DIR, \"04-trash/\"),\n",
        "    \"COMPLETED_ARCHIVES_DIR\": os.path.join(BASE_DIR, \"05-COMPLETED-ARCHIVES/\"),\n",
        "}\n",
        "CONFIG[\"QUARANTINE_DIR\"] = os.path.join(CONFIG[\"TRASH_DIR\"], \"quarantined_artifacts/\")\n",
        "CONFIG[\"DUPES_DIR\"] = os.path.join(CONFIG[\"TRASH_DIR\"], \"duplicates/\")\n",
        "VM_TEMP_EXTRACT_DIR = '/content/temp_extract'\n",
        "VM_TEMP_BATCH_DIR = '/content/temp_batch_creation'\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 2. WORKFLOW FUNCTIONS & HELPERS ---\n",
        "# ==============================================================================\n",
        "def dummy_tqdm(iterable, *args, **kwargs):\n",
        "    \"\"\"A dummy tqdm function that acts as a fallback if tqdm is not available.\"\"\"\n",
        "    description = kwargs.get('desc', 'items')\n",
        "    print(f\"    > Processing {description}...\")\n",
        "    return iterable\n",
        "\n",
        "def initialize_database(db_path):\n",
        "    \"\"\"Creates the SQLite database and the necessary tables if they don't exist.\"\"\"\n",
        "    print(\"--> [DB] Initializing database...\")\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS files (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        sha256_hash TEXT NOT NULL UNIQUE,\n",
        "        original_path TEXT NOT NULL,\n",
        "        final_path TEXT,\n",
        "        timestamp INTEGER,\n",
        "        file_size INTEGER NOT NULL,\n",
        "        source_archive TEXT NOT NULL\n",
        "    )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "    print(\"    ✅ Database ready.\")\n",
        "    return conn\n",
        "\n",
        "def perform_startup_integrity_check():\n",
        "    \"\"\"Performs a \"Power-On Self-Test\" to detect and correct inconsistent states.\"\"\"\n",
        "    print(\"--> [POST] Performing startup integrity check...\")\n",
        "    staging_dir = CONFIG[\"PROCESSING_STAGING_DIR\"]\n",
        "    source_dir = CONFIG[\"SOURCE_ARCHIVES_DIR\"]\n",
        "    if os.path.exists(staging_dir) and os.listdir(staging_dir):\n",
        "        print(\"    ⚠️ Found orphaned files. Rolling back transaction...\")\n",
        "        for orphan_file in os.listdir(staging_dir):\n",
        "            shutil.move(os.path.join(staging_dir, orphan_file), os.path.join(source_dir, orphan_file))\n",
        "        if os.path.exists(VM_TEMP_EXTRACT_DIR):\n",
        "            shutil.rmtree(VM_TEMP_EXTRACT_DIR)\n",
        "        print(\"    ✅ Rollback complete.\")\n",
        "    for filename in os.listdir(source_dir):\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "        if os.path.isfile(file_path) and os.path.getsize(file_path) == 0:\n",
        "            print(f\"    ⚠️ Found 0-byte artifact: '{filename}'. Quarantining...\")\n",
        "            shutil.move(file_path, os.path.join(CONFIG[\"QUARANTINE_DIR\"], filename))\n",
        "    print(\"--> [POST] Integrity check complete.\")\n",
        "\n",
        "def plan_and_repackage_archive(archive_path, dest_dir, conn, tqdm_module):\n",
        "    \"\"\"\n",
        "    Scans a large archive, hashes unique files, and repackages them into smaller,\n",
        "    crash-resilient parts while populating the database.\n",
        "    \"\"\"\n",
        "    original_basename_no_ext = os.path.splitext(os.path.basename(archive_path))[0]\n",
        "    print(f\"--> Repackaging & Indexing '{os.path.basename(archive_path)}'...\")\n",
        "    repackage_chunk_size_bytes = REPACKAGE_CHUNK_SIZE_GB * (1024**3)\n",
        "    cursor = conn.cursor()\n",
        "    try:\n",
        "        cursor.execute(\"SELECT sha256_hash FROM files\")\n",
        "        existing_hashes = {row[0] for row in cursor.fetchall()}\n",
        "        print(f\"    > Index loaded with {len(existing_hashes)} records.\")\n",
        "        existing_parts = [f for f in os.listdir(dest_dir) if f.startswith(original_basename_no_ext) and '.part-' in f]\n",
        "        last_part_num = 0\n",
        "        if existing_parts:\n",
        "            part_numbers = [int(re.search(r'\\.part-(\\d+)\\.tgz', f).group(1)) for f in existing_parts if re.search(r'\\.part-(\\d+)\\.tgz', f)]\n",
        "            if part_numbers: last_part_num = max(part_numbers)\n",
        "            print(f\"    ✅ Resuming after part {last_part_num}.\")\n",
        "        batches, current_batch, current_batch_size = [], [], 0\n",
        "        with tarfile.open(archive_path, 'r:gz') as original_tar:\n",
        "            all_members = [m for m in original_tar.getmembers() if m.isfile()]\n",
        "            for member in all_members:\n",
        "                if current_batch and (current_batch_size + member.size) > repackage_chunk_size_bytes:\n",
        "                    batches.append(current_batch); current_batch, current_batch_size = [], 0\n",
        "                current_batch.append(member); current_batch_size += member.size\n",
        "            if current_batch: batches.append(current_batch)\n",
        "        print(f\"    > Plan complete: {len(all_members)} files -> up to {len(batches)} new archives.\")\n",
        "        for i, batch in enumerate(batches):\n",
        "            part_number = i + 1\n",
        "            if part_number <= last_part_num: continue\n",
        "            os.makedirs(VM_TEMP_BATCH_DIR, exist_ok=True)\n",
        "            temp_batch_archive_path = os.path.join(VM_TEMP_BATCH_DIR, f\"temp_batch_{part_number}.tgz\")\n",
        "            files_in_this_batch = 0\n",
        "            with tarfile.open(temp_batch_archive_path, 'w:gz') as temp_tar:\n",
        "                with tarfile.open(archive_path, 'r:gz') as original_tar_stream:\n",
        "                    for member in tqdm_module(batch, desc=f\"Hashing & Staging Batch {part_number}\"):\n",
        "                        file_obj = original_tar_stream.extractfile(member)\n",
        "                        if not file_obj: continue\n",
        "                        file_content = file_obj.read()\n",
        "                        sha256 = hashlib.sha256(file_content).hexdigest()\n",
        "                        if sha256 in existing_hashes: continue\n",
        "                        file_obj.seek(0)\n",
        "                        temp_tar.addfile(member, file_obj)\n",
        "                        cursor.execute(\"INSERT INTO files (sha256_hash, original_path, file_size, source_archive) VALUES (?, ?, ?, ?)\",\n",
        "                                       (sha256, member.name, member.size, os.path.basename(archive_path)))\n",
        "                        existing_hashes.add(sha256)\n",
        "                        files_in_this_batch += 1\n",
        "            if files_in_this_batch > 0:\n",
        "                conn.commit()\n",
        "                new_archive_name = f\"{original_basename_no_ext}.part-{part_number:02d}.tgz\"\n",
        "                final_dest_path = os.path.join(dest_dir, new_archive_name)\n",
        "                print(f\"    --> Batch contains {files_in_this_batch} unique files. Moving final part {part_number} to Google Drive...\")\n",
        "                shutil.move(temp_batch_archive_path, final_dest_path)\n",
        "                print(f\"    ✅ Finished part {part_number}.\")\n",
        "            else:\n",
        "                print(f\"    > Batch {part_number} contained no unique files. Discarding empty batch.\")\n",
        "        return True\n",
        "    except Exception as e: print(f\"\\n❌ ERROR during JIT repackaging: {e}\"); traceback.print_exc(); return False\n",
        "    finally:\n",
        "        if os.path.exists(VM_TEMP_BATCH_DIR): shutil.rmtree(VM_TEMP_BATCH_DIR)\n",
        "\n",
        "def process_regular_archive(archive_path, conn, tqdm_module):\n",
        "    \"\"\"\n",
        "    Handles a regular-sized archive with Rsync-like file-level resumption by checking the DB\n",
        "    for already processed files and only unpacking the missing ones (\"the delta\").\n",
        "    \"\"\"\n",
        "    archive_name = os.path.basename(archive_path)\n",
        "    print(f\"\\n--> [Step 2a] Processing transaction for '{archive_name}'...\")\n",
        "    cursor = conn.cursor()\n",
        "    try:\n",
        "        print(\"    > Checking database for previously completed files...\")\n",
        "        cursor.execute(\"SELECT original_path FROM files WHERE source_archive = ? AND final_path IS NOT NULL\", (archive_name,))\n",
        "        completed_files = {row[0] for row in cursor.fetchall()}\n",
        "        if completed_files:\n",
        "            print(f\"    ✅ Found {len(completed_files)} completed files. Will skip unpacking them.\")\n",
        "        else:\n",
        "            print(\"    > No completed files found for this archive.\")\n",
        "\n",
        "        os.makedirs(VM_TEMP_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "        files_to_process = []\n",
        "        with tarfile.open(archive_path, 'r:gz') as tf:\n",
        "            all_members = [m for m in tf.getmembers() if m.isfile()]\n",
        "            for member in tqdm_module(all_members, desc=f\"Scanning {archive_name}\"):\n",
        "                if member.name not in completed_files:\n",
        "                    tf.extract(member, path=VM_TEMP_EXTRACT_DIR, set_attrs=False)\n",
        "                    files_to_process.append(member.name)\n",
        "\n",
        "        if not files_to_process:\n",
        "            print(\"    ✅ All files in this archive were already processed. Finalizing.\")\n",
        "            return True\n",
        "\n",
        "        print(f\"    ✅ Unpacked {len(files_to_process)} new/missing files to VM.\")\n",
        "\n",
        "        # Subsequent steps now operate on the much smaller \"delta\" of new files.\n",
        "        organize_photos(VM_TEMP_EXTRACT_DIR, CONFIG[\"ORGANIZED_DIR\"], conn, tqdm_module, archive_name)\n",
        "        deduplicate_files(VM_TEMP_EXTRACT_DIR, CONFIG[\"ORGANIZED_DIR\"], CONFIG[\"DUPES_DIR\"], conn, archive_name)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERROR during archive processing transaction: {e}\"); traceback.print_exc()\n",
        "        return False\n",
        "    finally:\n",
        "        if os.path.exists(VM_TEMP_EXTRACT_DIR):\n",
        "            shutil.rmtree(VM_TEMP_EXTRACT_DIR)\n",
        "            print(\"    > Cleaned up temporary VM directory.\")\n",
        "\n",
        "def deduplicate_files(source_path, primary_storage_path, trash_path, conn, source_archive_name):\n",
        "    \"\"\"Deduplicates new files against the primary storage and updates the database for unique files.\"\"\"\n",
        "    print(\"\\n--> [Step 2b] Deduplicating new files...\")\n",
        "    if not shutil.which('jdupes'):\n",
        "        print(\"    ⚠️ 'jdupes' not found. Skipping.\")\n",
        "        return\n",
        "\n",
        "    command = f'jdupes -r -S -nh --linkhard --move=\"{trash_path}\" \"{source_path}\" \"{primary_storage_path}\"'\n",
        "    subprocess.run(command, shell=True, capture_output=True)\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "    files_moved = 0\n",
        "    # The actual uncompressed data is often in a \"Takeout\" subdirectory.\n",
        "    takeout_source = os.path.join(source_path, \"Takeout\")\n",
        "    if os.path.exists(takeout_source):\n",
        "        print(\"    > Moving unique new files and updating database...\")\n",
        "        for root, _, files in os.walk(takeout_source):\n",
        "            for filename in files:\n",
        "                # This loop now handles all non-photo unique files left after jdupes.\n",
        "                unique_file_path = os.path.join(root, filename)\n",
        "                relative_path = os.path.relpath(unique_file_path, takeout_source)\n",
        "                final_path = os.path.join(primary_storage_path, relative_path)\n",
        "\n",
        "                os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
        "                shutil.move(unique_file_path, final_path)\n",
        "\n",
        "                # We need the original path relative to the TAR root, not the temp dir root.\n",
        "                original_path = os.path.relpath(unique_file_path, source_path)\n",
        "                cursor.execute(\"UPDATE files SET final_path = ? WHERE original_path = ? AND source_archive = ?\",\n",
        "                               (final_path, original_path, source_archive_name))\n",
        "                files_moved += 1\n",
        "        if files_moved > 0:\n",
        "            conn.commit()\n",
        "    print(f\"    ✅ Deduplication and move complete. Processed {files_moved} unique files.\")\n",
        "\n",
        "def organize_photos(source_path, final_dir, conn, tqdm_module, source_archive_name):\n",
        "    \"\"\"Organizes photos and updates their final path and timestamp in the database.\"\"\"\n",
        "    print(\"\\n--> [Step 2c] Organizing photos...\")\n",
        "    photos_organized_count = 0\n",
        "    photos_source_path = os.path.join(source_path, \"Takeout\", \"Google Photos\")\n",
        "    if not os.path.isdir(photos_source_path):\n",
        "        print(\"    > 'Google Photos' directory not found. Skipping.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        existing_filenames = set(os.listdir(final_dir))\n",
        "    except FileNotFoundError:\n",
        "        existing_filenames = set()\n",
        "\n",
        "    all_json_files = [os.path.join(r, f) for r, _, files in os.walk(photos_source_path) for f in files if f.lower().endswith('.json')]\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    for json_path in tqdm_module(all_json_files, desc=\"Organizing photos\"):\n",
        "        media_path = os.path.splitext(json_path)[0]\n",
        "        if not os.path.exists(media_path):\n",
        "            continue\n",
        "        try:\n",
        "            with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            ts = data.get('photoTakenTime', {}).get('timestamp')\n",
        "            if not ts:\n",
        "                continue\n",
        "\n",
        "            dt = datetime.fromtimestamp(int(ts))\n",
        "            new_name_base = dt.strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
        "            ext = os.path.splitext(media_path)[1].lower()\n",
        "            new_filename = f\"{new_name_base}{ext}\"\n",
        "            counter = 0\n",
        "            while new_filename in existing_filenames:\n",
        "                counter += 1\n",
        "                new_filename = f\"{new_name_base}_{counter}{ext}\"\n",
        "\n",
        "            final_filepath = os.path.join(final_dir, new_filename)\n",
        "            shutil.move(media_path, final_filepath)\n",
        "\n",
        "            original_path = os.path.relpath(media_path, source_path)\n",
        "            cursor.execute(\"UPDATE files SET final_path = ?, timestamp = ? WHERE original_path = ? AND source_archive = ?\",\n",
        "                           (final_filepath, int(ts), original_path, source_archive_name))\n",
        "\n",
        "            existing_filenames.add(new_filename)\n",
        "            photos_organized_count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if photos_organized_count > 0:\n",
        "        conn.commit()\n",
        "        print(f\"    ✅ Organized and updated DB for {photos_organized_count} new files.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- MAIN EXECUTION SCRIPT ---\n",
        "# ==============================================================================\n",
        "def main(args):\n",
        "    conn = None\n",
        "    try:\n",
        "        print(\"--> [Step 0] Initializing environment...\")\n",
        "        try: from tqdm.notebook import tqdm\n",
        "        except ImportError: tqdm = dummy_tqdm\n",
        "        from google.colab import drive\n",
        "        if not os.path.exists('/content/drive/MyDrive'): drive.mount('/content/drive')\n",
        "        else: print(\"✅ Google Drive already mounted.\")\n",
        "\n",
        "        for path in CONFIG.values(): os.makedirs(path, exist_ok=True)\n",
        "        conn = initialize_database(DB_PATH)\n",
        "        perform_startup_integrity_check()\n",
        "\n",
        "        total_vm, used_vm, free_vm = shutil.disk_usage('/content/')\n",
        "        used_vm_gb = used_vm / (1024**3)\n",
        "        if used_vm_gb > TARGET_MAX_VM_USAGE_GB:\n",
        "            print(f\"\\n❌ PRE-FLIGHT CHECK FAILED: Initial disk usage ({used_vm_gb:.2f} GB) is too high.\")\n",
        "            print(\"    >>> PLEASE RESTART THE COLAB RUNTIME (Runtime -> Restart runtime) AND TRY AGAIN. <<<\")\n",
        "            return\n",
        "        print(f\"✅ Pre-flight disk check passed. (Initial Usage: {used_vm_gb:.2f} GB)\")\n",
        "        subprocess.run(\"apt-get -qq install jdupes\", shell=True)\n",
        "        print(\"✅ Workspace ready.\")\n",
        "\n",
        "        print(\"\\n--> [Step 1] Identifying unprocessed archives...\")\n",
        "        all_archives = set(os.listdir(CONFIG[\"SOURCE_ARCHIVES_DIR\"]))\n",
        "        completed_archives = set(os.listdir(CONFIG[\"COMPLETED_ARCHIVES_DIR\"]))\n",
        "        processing_queue = sorted(list(all_archives - completed_archives))\n",
        "\n",
        "        if not processing_queue:\n",
        "            print(\"\\n✅🎉 All archives have been processed!\")\n",
        "        else:\n",
        "            archive_name = processing_queue[0]\n",
        "            source_path = os.path.join(CONFIG[\"SOURCE_ARCHIVES_DIR\"], archive_name)\n",
        "            staging_path = os.path.join(CONFIG[\"PROCESSING_STAGING_DIR\"], archive_name)\n",
        "\n",
        "            print(f\"--> Locking and staging '{archive_name}' for processing...\")\n",
        "            shutil.move(source_path, staging_path)\n",
        "\n",
        "            if os.path.getsize(staging_path) / (1024**3) > MAX_SAFE_ARCHIVE_SIZE_GB and '.part-' not in archive_name:\n",
        "                repackaging_ok = plan_and_repackage_archive(staging_path, CONFIG[\"SOURCE_ARCHIVES_DIR\"], conn, tqdm)\n",
        "                if repackaging_ok:\n",
        "                    shutil.move(staging_path, os.path.join(CONFIG[\"COMPLETED_ARCHIVES_DIR\"], archive_name))\n",
        "            else:\n",
        "                processed_ok = process_regular_archive(staging_path, conn, tqdm)\n",
        "                if processed_ok:\n",
        "                    shutil.move(staging_path, os.path.join(CONFIG[\"COMPLETED_ARCHIVES_DIR\"], archive_name))\n",
        "                else:\n",
        "                    print(f\"    ❌ Transaction failed for '{archive_name}'. It will be rolled back on the next run.\")\n",
        "\n",
        "            print(\"\\n--- WORKFLOW CYCLE COMPLETE. ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ A CRITICAL UNHANDLED ERROR OCCURRED: {e}\")\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "            print(\"--> [DB] Database connection closed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"Process Google Takeout archives.\")\n",
        "    parser.add_argument('--auto-delete-artifacts', action='store_true', help=\"Enable automatic deletion of 0-byte artifact files.\")\n",
        "    try:\n",
        "        args = parser.parse_args([])\n",
        "    except SystemExit:\n",
        "        args = parser.parse_args()\n",
        "    main(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdafksHl9jTotKTeGk6ISo"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}